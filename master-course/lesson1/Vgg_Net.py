from torch import nn

# 参考链接：https://blog.csdn.net/weixin_43467711/article/details/105377584


class Vgg16Net(nn.Module):
    def __init__(self):
        super(Vgg16Net, self).__init__()

        # 第一层，2个卷积层和一个最大池化层
        self.layer1 = nn.Sequential(
            # 输入3通道，卷积核3*3，输出64通道（如32*32*3的样本图片，(32+2*1-3)/1+1=32，输出32*32*64）
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            # 输入64通道，卷积核3*3，输出64通道（输入32*32*64，卷积3*3*64*64，输出32*32*64）
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            # 输入32*32*64，输出16*16*64
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        # 第二层，2个卷积层和一个最大池化层
        self.layer2 = nn.Sequential(
            # 输入64通道，卷积核3*3，输出128通道（输入16*16*64，卷积3*3*64*128，输出16*16*128）
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            # 输入128通道，卷积核3*3，输出128通道（输入16*16*128，卷积3*3*128*128，输出16*16*128）
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            # 输入16*16*128，输出8*8*128
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        # 第三层，3个卷积层和一个最大池化层
        self.layer3 = nn.Sequential(
            # 输入128通道，卷积核3*3，输出256通道（输入8*8*128，卷积3*3*128*256，输出8*8*256）
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            # 输入256通道，卷积核3*3，输出256通道（输入8*8*256，卷积3*3*256*256，输出8*8*256）
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            # 输入256通道，卷积核3*3，输出256通道（输入8*8*256，卷积3*3*256*256，输出8*8*256）
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            # 输入8*8*256，输出4*4*256
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        # 第四层，3个卷积层和1个最大池化层
        self.layer4 = nn.Sequential(
            # 输入256通道，卷积3*3，输出512通道（输入4*4*256，卷积3*3*256*512，输出4*4*512）
            nn.Conv2d(256, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            # 输入512通道，卷积3*3，输出512通道（输入4*4*512，卷积3*3*512*512，输出4*4*512）
            nn.Conv2d(512, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            # 输入512通道，卷积3*3，输出512通道（输入4*4*512，卷积3*3*512*512，输出4*4*512）
            nn.Conv2d(512, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            # 输入4*4*512，输出2*2*512
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        # 第五层，3个卷积层和1个最大池化层
        self.layer5 = nn.Sequential(
            # 输入512通道，卷积3*3，输出512通道（输入2*2*512，卷积3*3*512*512，输出2*2*512）
            nn.Conv2d(512, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            # 输入512通道，卷积3*3，输出512通道（输入2*2*512，卷积3*3*512*512，输出2*2*512）
            nn.Conv2d(512, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            # 输入512通道，卷积3*3，输出512通道（输入2*2*512，卷积3*3*512*512，输出2*2*512）
            nn.Conv2d(512, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            # 输入2*2*512，输出1*1*512
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.conv_layer = nn.Sequential(
            self.layer1,
            self.layer2,
            self.layer3,
            self.layer4,
            self.layer5
        )

        self.fc = nn.Sequential(
            nn.Linear(512, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(),

            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(),

            nn.Linear(256, 10)
        )
        # 输出(batchSize,10)

    def forward(self, x):
        x = self.conv_layer(x)  # 卷积输出(batchsize，channels，x，y)(b,c,h,w)(32,512,1,1)
        x = x.view(-1, 512)  # 将卷积结果展平为一维为(32,512)
        x = self.fc(x)  # 全连接层，输出[32,10]
        """
        [[-0.0266,  0.5358, -0.3468, -0.1389, -0.1288,  0.1302,  0.1038,  0.1826,
          0.2703,  0.1062],
        [-0.2149, -0.0244,  0.0479, -0.0068, -0.2742,  0.0387,  0.2560,  0.0376,
          0.2295, -0.1721],
        [-0.1498, -0.1273, -0.4098,  0.0313, -0.5201,  0.2215, -0.0673,  0.1680,
         -0.1639, -0.4174],
        [-0.0640, -0.0261, -0.5154,  0.2030, -0.1493, -0.0703,  0.2868,  0.2170,
          0.1241, -0.1331],
        [-0.0161,  0.0281, -0.3022, -0.0332, -0.0155, -0.0289,  0.2279,  0.4005,
          0.2232, -0.2622],
        [ 0.2364,  0.2173, -0.6466, -0.0405, -0.1710,  0.2120,  0.2642,  0.2840,
          0.4728, -0.4197],
        [ 0.0879,  0.5019, -0.2392, -0.4850, -0.1819,  0.3153,  0.6553, -0.1862,
          0.2969,  0.6380],
        [ 0.1018,  0.0263, -0.1556,  0.2929, -0.1174,  0.0699, -0.0149,  0.1355,
         -0.1446,  0.1433],
        [-0.3238, -0.2502, -0.1036,  0.2361, -0.2662,  0.0756,  0.2014,  0.0312,
          0.3952,  0.3009],
        [-0.1350,  0.1339, -0.3466, -0.0710, -0.4508,  0.1587,  0.0382,  0.5787,
          0.4512,  0.2845],
        [ 0.1281,  0.1061, -0.2931,  0.1341, -0.1897, -0.1002,  0.0475,  0.1717,
         -0.0076, -0.2419],
        [-0.3958,  0.0850,  0.0747, -0.1007, -0.0377,  0.0474,  0.0908,  0.2405,
          0.2466,  0.0971],
        [-0.3176,  0.1415, -0.2624, -0.0157, -0.1046,  0.0581,  0.2433,  0.1292,
         -0.0959, -0.2234],
        [-0.0234,  0.2162,  0.0726, -0.0264, -0.2789,  0.2553,  0.1036,  0.3583,
          0.2102, -0.1269],
        [-0.0701,  0.0391, -0.3510, -0.2412, -0.0931, -0.1524,  0.2578, -0.1859,
          0.2260, -0.2094],
        [ 0.4602,  0.0532, -0.6286, -0.1957, -0.5670,  0.4385,  0.3584,  0.1377,
         -0.0484,  0.3123],
        [-0.0969,  0.0412, -0.1346, -0.0923, -0.1049,  0.1889,  0.1183, -0.0882,
         -0.0077, -0.0329],
        [-0.1104,  0.1053, -0.2942,  0.2716, -0.2655, -0.0145, -0.2775,  0.2698,
          0.0997,  0.1039],
        [-0.1129,  0.0689, -0.0250, -0.2881, -0.0927,  0.0124,  0.4238,  0.1465,
          0.2668,  0.0835],
        [-0.2694,  0.3224, -0.1559,  0.0255, -0.0972,  0.0797,  0.0306,  0.4332,
          0.0708, -0.2640],
        [-0.2616,  0.0881, -0.3115, -0.1832,  0.0900,  0.1409, -0.0051, -0.1406,
          0.0271,  0.1856],
        [-0.4518,  0.5644, -0.2829,  0.3016, -0.3686,  0.1036, -0.1915, -0.0955,
          0.0654,  0.1372],
        [-0.0369,  0.1134, -0.2771, -0.0593, -0.2686, -0.1298,  0.2715, -0.2984,
          0.1061, -0.1124],
        [-0.4889, -0.0904, -0.2117, -0.1932, -0.4522,  0.3583, -0.2878,  0.0865,
          0.1668, -0.2309],
        [-0.2477,  0.1430,  0.0178, -0.0437, -0.0444,  0.0692,  0.1722,  0.2627,
          0.1049,  0.0860],
        [-0.0569, -0.3431, -0.0428, -0.2178, -0.4421,  0.2615,  0.3176,  0.1864,
          0.4022,  0.1547],
        [ 0.0599, -0.0488, -0.2485, -0.0268, -0.0656,  0.0416,  0.2021,  0.1802,
         -0.0341,  0.2157],
        [ 0.2398, -0.2537, -0.1936,  0.5982, -0.2710, -0.0850,  0.2154,  0.2468,
         -0.0176,  0.0167],
        [-0.2395,  0.3046,  0.1719,  0.4087,  0.2399, -0.0062,  0.1878,  0.6821,
          0.1283, -0.7710],
        [-0.3658, -0.3710, -0.2099, -0.1416,  0.1876,  0.0602,  0.6381, -0.0795,
          0.4198,  0.3850],
        [-0.4488,  0.3403, -0.4576, -0.1742, -0.4726,  0.1484,  0.3400,  0.3729,
          0.2505, -0.1280],
        [ 0.2261,  0.4476, -0.1350,  0.2389, -0.2699,  0.2459,  0.2519,  0.3733,
          0.1752,  0.3251]]
        """
        return x